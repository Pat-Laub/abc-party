---
title: Approximate Bayesian Computation and Insurance
author: Dr Patrick Laub <br> Joint work with Dr Pierre-Olivier Goffard
date: 3 February 2023
date-format: long
format:
  revealjs:
    theme: [serif, custom.scss]
    controls: true
    controls-tutorial: true
    logo: unsw-logo.svg
    title-slide-attributes:
      data-background-image: unsw-yellow-shape.png
      data-background-size: contain !important
    slide-number: c/t
    strip-comments: true
    margin: 0.2
    chalkboard:
      boardmarker-width: 5
      grid: false
    include-before: <div class="line right"></div>
highlight-style: breeze
execute:
  echo: true
---

# 

<h2>About me</h2>

::: columns
::: {.column width="65%"}

<br>

- Software engineer & maths (UQ)
- PhD in probability (Aarhus)
- Post-doc (ISFA Lyon)
- RSE (Uni Melbourne)
- Lecturer (UNSW)

:::
::: {.column width="35%"}

![](hawkes-book-cover.jpeg)

:::
:::

::: footer
Cf. [https://pat-laub.github.io](https://pat-laub.github.io).
:::

## {data-visibility="uncounted"}

<iframe src="https://pat-laub.github.io/DeepLearningForActuaries/" width="100%" height="700" style="border:none;"></iframe>

::: footer
Cf. [https://pat-laub.github.io](https://pat-laub.github.io).
:::

## 

![Our IME paper](abstract.png)

Packages: _approxbayescomp_ for Python & R.

## Insurance Motivation

Have a random number of claims $N \sim p_N( \,\cdot\, ; \boldsymbol{\theta}_{\mathrm{freq}} )$.

Random claim sizes $U_1, \dots, U_N \sim f_U( \,\cdot\, ; \boldsymbol{\theta}_{\mathrm{sev}} )$.

We aggregate them somehow, like:

- aggregate claims: $X = \sum_{i=1}^N U_i$
- maximum claims: $X = \max_{i=1}^N U_i$
- stop-loss: $X = ( \sum_{i=1}^N U_i - c )_+$.

__Question__: Given a sample $X_1, \dots, X_n$ of the summaries, what is the $\boldsymbol{\theta} = (\boldsymbol{\theta}_{\mathrm{freq}}, \boldsymbol{\theta}_{\mathrm{sev}})$ which explains them?

E.g. a reinsurance contract

## Statistics with likelihoods

We can only find the likelihood for simple models.

$$ X_1, X_2 \overset{\mathrm{i.i.d.}}{\sim} f_X(\,\cdot\,) $$

$$ \Rightarrow X_1 + X_2 \sim ~ \texttt{Intractable likelihood}! $$

. . .

Have a sample of $n$ i.i.d. observations. As $n$ increases,
$$ p_{\boldsymbol{X}}(\boldsymbol{x} \mid \boldsymbol{\theta}) = \prod \text{Small things} \overset{\dagger}{=} 0, $$
or just takes a long time to compute, then $\texttt{Intractable}$ $\texttt{Likelihood}$!

. . .

_Usually it's still possible to simulate these things..._

# Approximate Bayesian Computation {background-image="unsw-yellow-shape.png"}

<center>

<br>

QR CODE HERE

<br>

[https://pat-laub.github.io/abc-party/game](https://pat-laub.github.io/abc-party/game)

</center>

{{< include _coin-flip-animations.qmd >}}

# Theory {data-visibility="uncounted" background-image="unsw-yellow-shape.png"}

## Does it work in theory? {.smaller}

__Proposition__: Say we have continuous data $\boldsymbol{x}_{\text{obs}}$, and our prior $\pi(\boldsymbol{\theta})$ has bounded support.

If for some $\epsilon \ge 0$ we have

$$ \sup\limits_{ (\boldsymbol{z}, \boldsymbol{\theta}) : \mathcal{D}(\boldsymbol{z}, \boldsymbol{x}_{\text{obs}} ) < \epsilon, \boldsymbol{\theta} \in \boldsymbol{ \Theta } } \pi( \boldsymbol{z} \mid \boldsymbol{ \theta }) < \infty $$

then for each $\boldsymbol{\theta} \in \boldsymbol{\Theta}$

$$ \lim_{\epsilon \to 0} \pi_\epsilon(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}}) = \pi(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}}) . $$

We sample the _approximate_/_ABC posterior_.

$$ \pi_\epsilon(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}}) \propto \pi(\theta) \times \mathbb{P}\bigl( \lVert \boldsymbol{x}_{\text{obs}}-\boldsymbol{x}^{\ast} \rVert \leq \epsilon \text{ where } \boldsymbol{x}^{\ast} \sim \boldsymbol{\theta} \bigr) , $$

but we care about the true posterior $\pi(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}})$.

::: footer
Rubio and Johansen (2013), _A simple approach to maximum intractable likelihood estimation_, Electronic Journal of Statistics.
:::

## Mixed data

We filled in some of the blanks for mixed data.
Our data was mostly continuous data but had an atom at 0.



Get
$$ \lim_{\epsilon \to 0} \pi_\epsilon(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}}) \pi(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}}) $$

when

$$ \mathcal{D}(\boldsymbol{z}, \boldsymbol{x}_{\text{obs}}) = \begin{cases}
            \mathcal{D}^+(\boldsymbol{z}^+, \boldsymbol{x}_{\text{obs}^+}) & \text{if } \# \text{Zeros}(\boldsymbol{z}) = \# \text{Zeros}(\boldsymbol{x}_{\text{obs}}) , \\
            \infty & \text{otherwise}.
            \end{cases}
$$

# Practice {data-visibility="uncounted" background-image="unsw-yellow-shape.png"}

## ABC Sequential Monte Carlo {.smaller}

- Input: data $\boldsymbol{x}_{\text{obs}}$, prior $\pi(\boldsymbol{\theta})$, distance $\mathcal{D}(\cdot,\cdot)$
            # of generations $G$, # of particles $K$
- Start with $\epsilon_0 = \infty$,  $\pi_0(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}}) = \pi(\boldsymbol{\theta})$
- For each generation $g = 1$ to $G$
  - For each particle $k = 1$ to $K$
    - Repeatedly:
      - Generate a guess $\boldsymbol{\theta}^{\ast}$ from $\pi_{g-1}(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}})$
      - Generate fake data $\boldsymbol{x}^{\ast}$ from $\boldsymbol{\theta}^{\ast}$
      - Stop when $\mathcal{D}(\boldsymbol{x}^\ast , \boldsymbol{x}_{\text{obs}}) < \epsilon_{g-1}$
    - Store $\boldsymbol{\theta}_k^g = \boldsymbol{\theta}^{\ast}$
  - Create a new threshold $\epsilon_g \le \epsilon_{g-1} $ and a new population by discarding particles with $\mathcal{D}(\boldsymbol{x}_k^g  , \boldsymbol{x}_{\text{obs}}) \ge \epsilon_{g}$ until the effective sample size is $K / 2$
  - Weight each particle by $w_k^g \propto \pi(\boldsymbol{\theta}_k^g) / \pi_{g-1}( \boldsymbol{\theta}_k^g \mid \boldsymbol{x}_{\text{obs}} )$
  - Create a KDE $\pi_g(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}})$ based on the surviving $( \boldsymbol{\theta}_k^g , w_k^g )$ particles

## Code demonstration

<center>
<a href="https://asciinema.org/a/Yq3kMPrzuqMbUJJPs5uVN8OeY?size=big" target="_blank">
<img data-lazy-loaded="" style="width:100%" data-src="https://asciinema.org/a/Yq3kMPrzuqMbUJJPs5uVN8OeY.svg">
</a>
</center>

## Cost (2021)

| Example      | Time           | Cost          |
|--------------|----------------|---------------|
| Dependent    | 45 s           | 3.5 ¢         |
| Censored     | 141 s          | 11.1 ¢        |
| Misspecified | 40 s           | 3.2 ¢         |
| Time-varying | ~~780 s~~          | ~~61.6 ¢~~       |
| Bivariate    | ~~269 s~~          | ~~21 ¢~~          |
| TOTAL:       | ~~21.25 m~~ 3.76 m | ~~$1.004~~ 17.8 ¢ |

On AWS c6g.16xlarge instance (64 ARM cores)