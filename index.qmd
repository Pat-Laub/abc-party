---
title: Approximate Bayesian Computation and Insurance
subtitle: PARTY Valencia
author: Dr Patrick Laub
date: 3 February 2023
date-format: long
format:
  revealjs:
    theme: [serif, custom.scss]
    controls: true
    controls-tutorial: true
    logo: unsw-logo.svg
    title-slide-attributes:
      data-background-image: unsw-yellow-shape.png
      data-background-size: contain !important
    slide-number: c/t
    strip-comments: true
    margin: 0.2
    chalkboard:
      boardmarker-width: 5
      grid: false
    include-before: <div class="line right"></div>
    include-after: <script>registerRevealCallbacks();</script>
highlight-style: breeze
execute:
  echo: true
---

## {data-visibility="uncounted"}

<br>

![](title.png)

# 

<h2>What is the bias?</h2>

We flip a coin three times and get:

::: columns
::: {.column width="33%"}

![**Heads**](aud-heads.jpeg)

:::
::: {.column width="33%"}

![**Tails**](aud-tails.jpeg)
:::
::: {.column width="33%"}

![**Heads**](aud-heads.jpeg)

:::
:::

What is $\theta = \mathbb{P}(\text{Heads})$?

<!-- 
## Bayesian approach

::: columns
::: column
![Prior](coin-prior.png)
:::
::: column

<iframe width="560" height="280" src="https://www.youtube.com/embed/P6pugawb7SM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="560" height="280" src="https://www.youtube.com/embed/tuXXubOkFuY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

:::
:::
-->


## Let's flip coins

<center>

<img src="qr-code.png" height="400px"/>

[https://pat-laub.github.io/abc-party/game](https://pat-laub.github.io/abc-party/game)

</center>

{{< include _coin-flip-animations.qmd >}}

#

<h2>Statistics with likelihoods</h2>

We can only find the likelihood for simple models.

$$ X_1, X_2 \overset{\mathrm{i.i.d.}}{\sim} f_X(\,\cdot\,) $$

$$ \Rightarrow X_1 + X_2 \sim ~ \texttt{Intractable likelihood}! $$

. . .

Have a sample of $n$ i.i.d. observations. As $n$ increases,
$$ p_{\boldsymbol{X}}(\boldsymbol{x} \mid \boldsymbol{\theta}) = \prod \text{Small things} \overset{\dagger}{=} 0, $$
or just takes a long time to compute, then $\texttt{Intractable}$ $\texttt{Likelihood}$!

. . .

_Usually it's still possible to simulate these things..._

## Insurance Motivation

Have a random number of claims $N \sim p_N( \,\cdot\, ; \boldsymbol{\theta}_{\mathrm{freq}} )$.

Random claim sizes $U_1, \dots, U_N \sim f_U( \,\cdot\, ; \boldsymbol{\theta}_{\mathrm{sev}} )$.

We aggregate them somehow, like:

- aggregate claims: $X = \sum_{i=1}^N U_i$
- maximum claims: $X = \max_{i=1}^N U_i$
- stop-loss: $X = ( \sum_{i=1}^N U_i - c )_+$.

__Question__: Given a sample $X_1, \dots, X_n$ of the summaries, what is the $\boldsymbol{\theta} = (\boldsymbol{\theta}_{\mathrm{freq}}, \boldsymbol{\theta}_{\mathrm{sev}})$ which explains them?

E.g. a reinsurance contract

<!-- # Examples {data-visibility="uncounted" background-image="unsw-yellow-shape.png"} -->

## Dependent random sums

::: {.smaller}
$$
N \sim \mathsf{Poisson}(\lambda), 
\quad U_i \mid N \sim \mathsf{Exp}(\beta\times \mathrm{e}^{\delta N}),
\quad X = \sum_{i=1}^N U_i.
$$
:::

![Posteriors for $\lambda$, $\beta$, and $\delta$ with <span style="color:rgb(31, 119, 180);">50 sums</span> and <span style="color:rgb(44, 160, 44);">250 sums</span>.](hist-test2-poisson-depexp.svg)

::: {.absolute top=230 left=200}
$\lambda$
:::

::: {.absolute top=230 left=500}
$\beta$
:::

::: {.absolute top=230 left=800}
$\delta$
:::

::: footer
J. Garrido, C. Genest, and J. Schulz (2016), _Generalized linear models for dependent frequency and severity of insurance claims_, IME.
:::

## Dependent random sums code

<br>

``` python
import approxbayescomp as abc
 
# Load data to fit
obsData = ...
 
# Frequency-Loss Model
freq = "poisson"
sev = "frequency dependent exponential"
psi = abc.Psi("sum") # Aggregation process
 
# Fit the model to the data using ABC
prior = abc.IndependentUniformPrior([(0, 10), (0, 20), (-1, 1)])
model = abc.Model(freq, sev, psi, prior)
fit = abc.smc(numIters, popSize, obsData, model)
```

## Time-varying arrival rate {data-visibility="uncounted"}

$$\lambda(t) = a+b[1+\sin(2\pi c t)]$$

![Intensity](cyclical_poisson_lognormal_data.svg)
![](legend-cyclical.svg)

## Time-varying example {data-visibility="uncounted"}

::: {.smaller}
Sizes are $U_i \sim \mathsf{Lognormal}(\mu, \sigma)$, observe $X_s = \sum_{i = N_{s-1}}^{N_{s}}U_i$.
:::

::: {.absolute top=150 left=100}
$a$
:::

::: {.absolute top=150 left=350}
$b$
:::

::: {.absolute top=150 left=550}
$c$
:::

::: {.absolute top=150 left=800}
$\mu$
:::

::: {.absolute top=150 left=1000}
$\sigma$
:::


![Posteriors for $a$, $b$, $c$, $\mu$, and $\sigma$ with <span style="color:rgb(31, 119, 180);">50 sums</span> and <span style="color:rgb(44, 160, 44);">250 sums</span>.](hist-cyclical-poisson-lnorm-curve_matching.svg)
   
# 

<h2>Python package</h2>

<iframe src="https://pat-laub.github.io/approxbayescomp/" width="100%" height="600" style="border:none;"></iframe>

## R package


<iframe src="https://pat-laub.github.io/approxbayescomp-r/" width="100%" height="600" style="border:none;"></iframe>

::: footer
A rough draft of C++ implementation: [https://github.com/Pat-Laub/cppabc](https://github.com/Pat-Laub/cppabc).
:::

## Stats problem $\to$ computer problem

::: {.r-stack}
![](mac-mini.png)

![](aws.png){.fragment}
:::

. . .

> "... you can just throw more and more computers at the problem, and that's much much easier than throwing more brains at a problem."
Hadley Wickham

## {.smaller}

<h2>Get involved!</h2>


::: columns
::: {.column width="55%"}

üòä Give it a try, feedback would be very welcome.

üòç We'd love contributions! 

![Empirical Dynamic Modelling](edm.svg)

Cf. [https://pat-laub.github.io](https://pat-laub.github.io).

:::
::: {.column width="45%"}
:::
:::

::: {.absolute top=-25 right=100 width=300}
![](hawkes-book-cover.jpeg)
:::

::: {.absolute top=450 right=0 width=500}
![Deep Learning for Actuaries](autoencoder.svg)
:::

# Appendix {data-visibility="uncounted" background-image="unsw-yellow-shape.png"}

## {data-visibility="uncounted"}

<iframe src="https://pat-laub.github.io/DeepLearningForActuaries/" width="100%" height="700" style="border:none;"></iframe>

## {.smaller data-visibility="uncounted"}

<h2>Does it work in theory?</h2>

__Proposition__: Say we have continuous data $\boldsymbol{x}_{\text{obs}}$, and our prior $\pi(\boldsymbol{\theta})$ has bounded support.

If for some $\epsilon \ge 0$ we have

$$ \sup\limits_{ (\boldsymbol{z}, \boldsymbol{\theta}) : \mathcal{D}(\boldsymbol{z}, \boldsymbol{x}_{\text{obs}} ) < \epsilon, \boldsymbol{\theta} \in \boldsymbol{ \Theta } } \pi( \boldsymbol{z} \mid \boldsymbol{ \theta }) < \infty $$

then for each $\boldsymbol{\theta} \in \boldsymbol{\Theta}$

$$ \lim_{\epsilon \to 0} \pi_\epsilon(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}}) = \pi(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}}) . $$

::: {.absolute top=380 left=1000}
$\square$
:::

We sample the _approximate_/_ABC posterior_.

$$ \pi_\epsilon(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}}) \propto \pi(\theta) \times \mathbb{P}\bigl( \mathcal{D}\bigl(\boldsymbol{x}_{\text{obs}}, \boldsymbol{x}^{\ast} \bigr) \leq \epsilon \text{ where } \boldsymbol{x}^{\ast} \sim \boldsymbol{\theta} \bigr) , $$

but we care about the true posterior $\pi(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}})$.

::: footer
Rubio and Johansen (2013), _A simple approach to maximum intractable likelihood estimation_, Electronic Journal of Statistics.
:::

## Mixed data {data-visibility="uncounted"}

We filled in some of the blanks for mixed data.
Our data was mostly continuous data but had an atom at 0.



Get
$$ \lim_{\epsilon \to 0} \pi_\epsilon(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}}) \pi(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}}) $$

when

$$ \mathcal{D}(\boldsymbol{z}, \boldsymbol{x}_{\text{obs}}) = \begin{cases}
            \mathcal{D}^+(\boldsymbol{z}^+, \boldsymbol{x}_{\text{obs}^+}) & \text{if } \# \text{Zeros}(\boldsymbol{z}) = \# \text{Zeros}(\boldsymbol{x}_{\text{obs}}) , \\
            \infty & \text{otherwise}.
            \end{cases}
$$

## ABC Sequential Monte Carlo {.smaller data-visibility="uncounted"}

- Input: data $\boldsymbol{x}_{\text{obs}}$, prior $\pi(\boldsymbol{\theta})$, distance $\mathcal{D}(\cdot,\cdot)$, # of generations $G$, # of particles $K$
- Start with $\epsilon_0 = \infty$,  $\pi_0(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}}) = \pi(\boldsymbol{\theta})$
- For each generation $g = 1$ to $G$
  - For each particle $k = 1$ to $K$
    - Repeatedly:
      - Generate a guess $\boldsymbol{\theta}^{\ast}$ from $\pi_{g-1}(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}})$
      - Generate fake data $\boldsymbol{x}^{\ast}$ from $\boldsymbol{\theta}^{\ast}$
      - Stop when $\mathcal{D}(\boldsymbol{x}^\ast , \boldsymbol{x}_{\text{obs}}) < \epsilon_{g-1}$
    - Store $\boldsymbol{\theta}_k^g = \boldsymbol{\theta}^{\ast}$
  - Create a new threshold $\epsilon_g \le \epsilon_{g-1}$ and a new population by discarding particles with $\mathcal{D}(\boldsymbol{x}_k^g  , \boldsymbol{x}_{\text{obs}}) \ge \epsilon_{g}$ until the effective sample size is $K / 2$
  - Weight each particle by $w_k^g \propto \pi(\boldsymbol{\theta}_k^g) / \pi_{g-1}( \boldsymbol{\theta}_k^g \mid \boldsymbol{x}_{\text{obs}} )$
  - Create a KDE $\pi_g(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}})$ based on the surviving $( \boldsymbol{\theta}_k^g , w_k^g )$ particles


<script defer>
    // Remove the highlight.js class for the 'compile', 'min', 'max'
    // as there's a bug where they are treated like the Python built-in
    // global functions but we only ever see it as methods like
    // 'model.compile()' or 'predictions.max()'
    buggyBuiltIns = ["compile", "min", "max", "round", "sum"];

    document.querySelectorAll('.bu').forEach((elem) => {
        if (buggyBuiltIns.includes(elem.innerHTML)) {
            elem.classList.remove('bu');
        }
    })

    var registerRevealCallbacks = function() {
        Reveal.on('overviewshown', event => {
            document.querySelector(".line.right").hidden = true;
        });
        Reveal.on('overviewhidden', event => {
            document.querySelector(".line.right").hidden = false;
        });
    };
</script>
