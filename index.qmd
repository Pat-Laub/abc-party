---
title: Approximate Bayesian Computation and Insurance
subtitle: PARTY Valencia
author: Dr Patrick Laub
date: 3 February 2023
date-format: long
format:
  revealjs:
    theme: [serif, custom.scss]
    controls: true
    controls-tutorial: true
    logo: unsw-logo.svg
    title-slide-attributes:
      data-background-image: unsw-yellow-shape.png
      data-background-size: contain !important
    slide-number: c/t
    strip-comments: true
    margin: 0.2
    chalkboard:
      boardmarker-width: 5
      grid: false
    include-before: <div class="line right"></div>
highlight-style: breeze
execute:
  echo: true
---

# {.smaller}

<h2 >About me</h2>

::: columns
::: {.column width="70%"}

- PhD @ Aarhus University & University of Queensland
- Post-docs @ ISFA Lyon & University of Melbourne
- Lecturer, UNSW

![Empirical Dynamic Modelling](edm.svg)

:::
::: {.column width="30%"}

![](hawkes-book-cover.jpeg)

:::
:::

::: footer
Cf. [https://pat-laub.github.io](https://pat-laub.github.io).
:::

## {data-visibility="uncounted"}

<iframe src="https://pat-laub.github.io/DeepLearningForActuaries/" width="100%" height="700" style="border:none;"></iframe>

::: footer
Cf. [https://pat-laub.github.io](https://pat-laub.github.io).
:::

## {data-visibility="uncounted"}

![Our IME paper](abstract.png)

## Insurance Motivation

Have a random number of claims $N \sim p_N( \,\cdot\, ; \boldsymbol{\theta}_{\mathrm{freq}} )$.

Random claim sizes $U_1, \dots, U_N \sim f_U( \,\cdot\, ; \boldsymbol{\theta}_{\mathrm{sev}} )$.

We aggregate them somehow, like:

- aggregate claims: $X = \sum_{i=1}^N U_i$
- maximum claims: $X = \max_{i=1}^N U_i$
- stop-loss: $X = ( \sum_{i=1}^N U_i - c )_+$.

__Question__: Given a sample $X_1, \dots, X_n$ of the summaries, what is the $\boldsymbol{\theta} = (\boldsymbol{\theta}_{\mathrm{freq}}, \boldsymbol{\theta}_{\mathrm{sev}})$ which explains them?

E.g. a reinsurance contract

## Statistics with likelihoods

We can only find the likelihood for simple models.

$$ X_1, X_2 \overset{\mathrm{i.i.d.}}{\sim} f_X(\,\cdot\,) $$

$$ \Rightarrow X_1 + X_2 \sim ~ \texttt{Intractable likelihood}! $$

. . .

Have a sample of $n$ i.i.d. observations. As $n$ increases,
$$ p_{\boldsymbol{X}}(\boldsymbol{x} \mid \boldsymbol{\theta}) = \prod \text{Small things} \overset{\dagger}{=} 0, $$
or just takes a long time to compute, then $\texttt{Intractable}$ $\texttt{Likelihood}$!

. . .

_Usually it's still possible to simulate these things..._

# ABC Game {background-image="unsw-yellow-shape.png"}

<center>

<img src="qr-code.png" height="400px"/>

[https://pat-laub.github.io/abc-party/game](https://pat-laub.github.io/abc-party/game)

</center>

{{< include _coin-flip-animations.qmd >}}

# In Theory & In Practice {data-visibility="uncounted" background-image="unsw-yellow-shape.png"}

## Does it work in theory? {.smaller}

__Proposition__: Say we have continuous data $\boldsymbol{x}_{\text{obs}}$, and our prior $\pi(\boldsymbol{\theta})$ has bounded support.

If for some $\epsilon \ge 0$ we have

$$ \sup\limits_{ (\boldsymbol{z}, \boldsymbol{\theta}) : \mathcal{D}(\boldsymbol{z}, \boldsymbol{x}_{\text{obs}} ) < \epsilon, \boldsymbol{\theta} \in \boldsymbol{ \Theta } } \pi( \boldsymbol{z} \mid \boldsymbol{ \theta }) < \infty $$

then for each $\boldsymbol{\theta} \in \boldsymbol{\Theta}$

$$ \lim_{\epsilon \to 0} \pi_\epsilon(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}}) = \pi(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}}) . $$

::: {.absolute top=380 left=1000}
$\square$
:::

We sample the _approximate_/_ABC posterior_.

$$ \pi_\epsilon(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}}) \propto \pi(\theta) \times \mathbb{P}\bigl( \mathcal{D}\bigl(\boldsymbol{x}_{\text{obs}}, \boldsymbol{x}^{\ast} \bigr) \leq \epsilon \text{ where } \boldsymbol{x}^{\ast} \sim \boldsymbol{\theta} \bigr) , $$

but we care about the true posterior $\pi(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}})$.

::: footer
Rubio and Johansen (2013), _A simple approach to maximum intractable likelihood estimation_, Electronic Journal of Statistics.
:::

## Mixed data

We filled in some of the blanks for mixed data.
Our data was mostly continuous data but had an atom at 0.



Get
$$ \lim_{\epsilon \to 0} \pi_\epsilon(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}}) \pi(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}}) $$

when

$$ \mathcal{D}(\boldsymbol{z}, \boldsymbol{x}_{\text{obs}}) = \begin{cases}
            \mathcal{D}^+(\boldsymbol{z}^+, \boldsymbol{x}_{\text{obs}^+}) & \text{if } \# \text{Zeros}(\boldsymbol{z}) = \# \text{Zeros}(\boldsymbol{x}_{\text{obs}}) , \\
            \infty & \text{otherwise}.
            \end{cases}
$$

## ABC Sequential Monte Carlo {.smaller}

- Input: data $\boldsymbol{x}_{\text{obs}}$, prior $\pi(\boldsymbol{\theta})$, distance $\mathcal{D}(\cdot,\cdot)$, # of generations $G$, # of particles $K$
- Start with $\epsilon_0 = \infty$,  $\pi_0(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}}) = \pi(\boldsymbol{\theta})$
- For each generation $g = 1$ to $G$
  - For each particle $k = 1$ to $K$
    - Repeatedly:
      - Generate a guess $\boldsymbol{\theta}^{\ast}$ from $\pi_{g-1}(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}})$
      - Generate fake data $\boldsymbol{x}^{\ast}$ from $\boldsymbol{\theta}^{\ast}$
      - Stop when $\mathcal{D}(\boldsymbol{x}^\ast , \boldsymbol{x}_{\text{obs}}) < \epsilon_{g-1}$
    - Store $\boldsymbol{\theta}_k^g = \boldsymbol{\theta}^{\ast}$
  - Create a new threshold $\epsilon_g \le \epsilon_{g-1}$ and a new population by discarding particles with $\mathcal{D}(\boldsymbol{x}_k^g  , \boldsymbol{x}_{\text{obs}}) \ge \epsilon_{g}$ until the effective sample size is $K / 2$
  - Weight each particle by $w_k^g \propto \pi(\boldsymbol{\theta}_k^g) / \pi_{g-1}( \boldsymbol{\theta}_k^g \mid \boldsymbol{x}_{\text{obs}} )$
  - Create a KDE $\pi_g(\boldsymbol{\theta} \mid \boldsymbol{x}_{\text{obs}})$ based on the surviving $( \boldsymbol{\theta}_k^g , w_k^g )$ particles

# Examples {data-visibility="uncounted" background-image="unsw-yellow-shape.png"}

## Dependent random sums 

::: {.smaller}
$$
N \sim \mathsf{Poisson}(\lambda), 
\quad U_i \mid N \sim \mathsf{Exp}(\beta\times \mathrm{e}^{\delta N}),
\quad X = \sum_{i=1}^N U_i.
$$
:::

![Posteriors for $\lambda$, $\beta$, and $\delta$ with <span style="color:rgb(31, 119, 180);">50 sums</span> and <span style="color:rgb(44, 160, 44);">250 sums</span>.](hist-test2-poisson-depexp.svg)

::: {.absolute top=230 left=200}
$\lambda$
:::

::: {.absolute top=230 left=500}
$\beta$
:::

::: {.absolute top=230 left=800}
$\delta$
:::

::: footer
J. Garrido, C. Genest, and J. Schulz (2016), _Generalized linear models for dependent frequency and severity of insurance claims_, IME.
:::

## Dependent random sums code

<br>

``` python
import approxbayescomp as abc
 
# Load data to fit
obsData = ...
 
# Frequency-Loss Model
freq = "poisson"
sev = "frequency dependent exponential"
psi = abc.Psi("sum") # Aggregation process
 
# Fit the model to the data using ABC
prior = abc.IndependentUniformPrior([(0, 10), (0, 20), (-1, 1)])
model = abc.Model(freq, sev, psi, prior)
fit = abc.smc(numIters, popSize, obsData, model)
```

## Time-varying arrival rate {data-visibility="uncounted"}

$$\lambda(t) = a+b[1+\sin(2\pi c t)]$$

![Intensity](cyclical_poisson_lognormal_data.svg)
![](legend-cyclical.svg)

## Time-varying example {data-visibility="uncounted"}

::: {.smaller}
Sizes are $U_i \sim \mathsf{Lognormal}(\mu, \sigma)$, observe $X_s = \sum_{i = N_{s-1}}^{N_{s}}U_i$.
:::

::: {.absolute top=150 left=100}
$a$
:::

::: {.absolute top=150 left=350}
$b$
:::

::: {.absolute top=150 left=550}
$c$
:::

::: {.absolute top=150 left=800}
$\mu$
:::

::: {.absolute top=150 left=1000}
$\sigma$
:::


![Posteriors for $a$, $b$, $c$, $\mu$, and $\sigma$ with <span style="color:rgb(31, 119, 180);">50 sums</span> and <span style="color:rgb(44, 160, 44);">250 sums</span>.](hist-cyclical-poisson-lnorm-curve_matching.svg)
   
## Python package

<iframe src="https://pat-laub.github.io/approxbayescomp/" width="100%" height="600" style="border:none;"></iframe>

## R package


<iframe src="https://pat-laub.github.io/approxbayescomp-r/" width="100%" height="600" style="border:none;"></iframe>

::: footer
A rough draft of C++ implementation: [https://github.com/Pat-Laub/cppabc](https://github.com/Pat-Laub/cppabc).
:::

<!-- ## Cost (2021)

| Example      | Time           | Cost          |
|--------------|----------------|---------------|
| Dependent    | 45 s           | 3.5 ¢         |
| Censored     | 141 s          | 11.1 ¢        |
| Misspecified | 40 s           | 3.2 ¢         |
| Time-varying | ~~780 s~~          | ~~61.6 ¢~~       |
| Bivariate    | ~~269 s~~          | ~~21 ¢~~          |
| TOTAL:       | ~~21.25 m~~ 3.76 m | ~~$1.004~~ 17.8 ¢ |

On AWS c6g.16xlarge instance (64 ARM cores) -->